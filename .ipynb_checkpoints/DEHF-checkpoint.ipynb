{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9651985-97a9-43ec-813f-250fd1d04e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.init as init\n",
    "\n",
    "class GNNLayer(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        # init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, features, adj, active=True):\n",
    "        support = torch.mm(features, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if active:\n",
    "            output = F.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4518adb5-64e3-4f5b-8380-8965b143f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import pairwise_distances as pair\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class load_data(Dataset):\n",
    "    def __init__(self, dataset, label):\n",
    "        self.x = dataset\n",
    "        self.y = label\n",
    "        # data_mat.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(np.array(self.x[idx])), \\\n",
    "            torch.from_numpy(np.array(self.y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4016d37e-b054-4aaa-9cdd-5775c4164f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (220, 21025)\n",
      "DTFU(\n",
      "  (ael): AE(\n",
      "    (enc_1): Linear(in_features=21025, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc_3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (z_layer): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dec_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec_3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (x_bar_layer): Linear(in_features=512, out_features=21025, bias=True)\n",
      "  )\n",
      "  (gnn_1): GNNLayer()\n",
      "  (gnn_2): GNNLayer()\n",
      "  (gnn_3): GNNLayer()\n",
      "  (gnn_4): GNNLayer()\n",
      "  (gnn_5): GNNLayer()\n",
      "  (gnn_7): GNNLayer()\n",
      "  (gnn_8): GNNLayer()\n",
      "  (gnn_9): GNNLayer()\n",
      "  (fuse1): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse2): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse3): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=256, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse4): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=128, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse5): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=256, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse6): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse7): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse8): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=42050, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "completed!\n",
      "0 loss: 352.9753723144531, re_loss:0.3515695035457611,re_graphloss:1.4058892726898193\n",
      "1 loss: 333.0823669433594, re_loss:0.33192703127861023,re_graphloss:1.1553384065628052\n",
      "2 loss: 317.93402099609375, re_loss:0.31677111983299255,re_graphloss:1.1628886461257935\n",
      "3 loss: 301.8150634765625, re_loss:0.3007511496543884,re_graphloss:1.0638903379440308\n",
      "4 loss: 284.2505798339844, re_loss:0.28321775794029236,re_graphloss:1.0328278541564941\n",
      "5 loss: 269.0376281738281, re_loss:0.26804906129837036,re_graphloss:0.9885595440864563\n",
      "6 loss: 256.03741455078125, re_loss:0.2550252377986908,re_graphloss:1.0121734142303467\n",
      "7 loss: 240.44522094726562, re_loss:0.23946157097816467,re_graphloss:0.9836400747299194\n",
      "8 loss: 226.61631774902344, re_loss:0.22566112875938416,re_graphloss:0.955187976360321\n",
      "9 loss: 213.31192016601562, re_loss:0.21236573159694672,re_graphloss:0.9461895823478699\n",
      "10 loss: 200.67564392089844, re_loss:0.19972114264965057,re_graphloss:0.954495906829834\n",
      "11 loss: 188.5926055908203, re_loss:0.18764054775238037,re_graphloss:0.9520507454872131\n",
      "12 loss: 177.3900604248047, re_loss:0.1764281988143921,re_graphloss:0.9618676900863647\n",
      "13 loss: 166.6592254638672, re_loss:0.16569924354553223,re_graphloss:0.9599814414978027\n",
      "14 loss: 156.5507354736328, re_loss:0.1555921584367752,re_graphloss:0.9585655331611633\n",
      "15 loss: 147.15390014648438, re_loss:0.14619216322898865,re_graphloss:0.9617371559143066\n",
      "16 loss: 138.13157653808594, re_loss:0.1371556669473648,re_graphloss:0.9759065508842468\n",
      "17 loss: 129.56307983398438, re_loss:0.1285848766565323,re_graphloss:0.9782118797302246\n",
      "18 loss: 121.40766143798828, re_loss:0.12044360488653183,re_graphloss:0.9640602469444275\n",
      "19 loss: 113.68279266357422, re_loss:0.11271946877241135,re_graphloss:0.9633253216743469\n",
      "20 loss: 106.4969253540039, re_loss:0.10553444921970367,re_graphloss:0.9624806642532349\n",
      "21 loss: 99.86494445800781, re_loss:0.0989047959446907,re_graphloss:0.960152804851532\n",
      "22 loss: 93.57089233398438, re_loss:0.09260839223861694,re_graphloss:0.9625025987625122\n",
      "23 loss: 87.68596649169922, re_loss:0.08672842383384705,re_graphloss:0.9575405120849609\n",
      "24 loss: 82.3166732788086, re_loss:0.08136439323425293,re_graphloss:0.9522783756256104\n",
      "25 loss: 77.33253479003906, re_loss:0.07637982070446014,re_graphloss:0.9527098536491394\n",
      "26 loss: 72.63504028320312, re_loss:0.07168246805667877,re_graphloss:0.9525764584541321\n",
      "27 loss: 68.24456024169922, re_loss:0.06729187816381454,re_graphloss:0.9526827931404114\n",
      "28 loss: 64.21651458740234, re_loss:0.06326334178447723,re_graphloss:0.9531757831573486\n",
      "29 loss: 60.45845031738281, re_loss:0.05950712785124779,re_graphloss:0.9513204097747803\n",
      "30 loss: 56.999698638916016, re_loss:0.056046053767204285,re_graphloss:0.9536451697349548\n",
      "31 loss: 53.7656135559082, re_loss:0.052813295274972916,re_graphloss:0.9523215889930725\n",
      "32 loss: 50.75593566894531, re_loss:0.049803245812654495,re_graphloss:0.9526911377906799\n",
      "33 loss: 47.97043228149414, re_loss:0.04701897129416466,re_graphloss:0.951461911201477\n",
      "34 loss: 45.413230895996094, re_loss:0.044465117156505585,re_graphloss:0.9481130242347717\n",
      "35 loss: 43.02490234375, re_loss:0.042085375636816025,re_graphloss:0.9395244121551514\n",
      "36 loss: 40.77101516723633, re_loss:0.03983541578054428,re_graphloss:0.9356017708778381\n",
      "37 loss: 38.67987060546875, re_loss:0.03774404153227806,re_graphloss:0.9358303546905518\n",
      "38 loss: 36.741573333740234, re_loss:0.03580830618739128,re_graphloss:0.9332646131515503\n",
      "39 loss: 34.91661834716797, re_loss:0.033988501876592636,re_graphloss:0.9281167387962341\n",
      "40 loss: 33.224464416503906, re_loss:0.03229900822043419,re_graphloss:0.9254584312438965\n",
      "41 loss: 31.647653579711914, re_loss:0.030723847448825836,re_graphloss:0.9238046407699585\n",
      "42 loss: 30.155981063842773, re_loss:0.029235754162073135,re_graphloss:0.9202277660369873\n",
      "43 loss: 28.735923767089844, re_loss:0.027823666110634804,re_graphloss:0.9122586250305176\n",
      "44 loss: 27.40612030029297, re_loss:0.02650064416229725,re_graphloss:0.9054761528968811\n",
      "45 loss: 26.175230026245117, re_loss:0.025270305573940277,re_graphloss:0.9049240350723267\n",
      "46 loss: 25.01419448852539, re_loss:0.02411377988755703,re_graphloss:0.9004161357879639\n",
      "47 loss: 23.91775131225586, re_loss:0.023018667474389076,re_graphloss:0.8990843296051025\n",
      "48 loss: 22.895877838134766, re_loss:0.021999243646860123,re_graphloss:0.896635115146637\n",
      "49 loss: 21.937400817871094, re_loss:0.02104651927947998,re_graphloss:0.8908807039260864\n",
      "50 loss: 21.036115646362305, re_loss:0.020150423049926758,re_graphloss:0.8856926560401917\n",
      "51 loss: 20.184484481811523, re_loss:0.01930304616689682,re_graphloss:0.8814396262168884\n",
      "52 loss: 19.372522354125977, re_loss:0.018494702875614166,re_graphloss:0.8778196573257446\n",
      "53 loss: 18.599538803100586, re_loss:0.017726173624396324,re_graphloss:0.8733659386634827\n",
      "54 loss: 17.87689781188965, re_loss:0.01700735092163086,re_graphloss:0.8695470094680786\n",
      "55 loss: 17.194351196289062, re_loss:0.016328927129507065,re_graphloss:0.8654239177703857\n",
      "56 loss: 16.546293258666992, re_loss:0.015684375539422035,re_graphloss:0.8619182109832764\n",
      "57 loss: 15.93303108215332, re_loss:0.015076015144586563,re_graphloss:0.8570157289505005\n",
      "58 loss: 15.357236862182617, re_loss:0.014503388665616512,re_graphloss:0.8538486361503601\n",
      "59 loss: 14.810807228088379, re_loss:0.013959809206426144,re_graphloss:0.8509975075721741\n",
      "60 loss: 14.292831420898438, re_loss:0.013444887474179268,re_graphloss:0.8479442000389099\n",
      "61 loss: 13.798579216003418, re_loss:0.012954204343259335,re_graphloss:0.8443748950958252\n",
      "62 loss: 13.329859733581543, re_loss:0.012489952147006989,re_graphloss:0.8399075865745544\n",
      "63 loss: 12.884221076965332, re_loss:0.012052331119775772,re_graphloss:0.8318901658058167\n",
      "64 loss: 12.460980415344238, re_loss:0.01163375936448574,re_graphloss:0.827221155166626\n",
      "65 loss: 12.05794620513916, re_loss:0.011232756078243256,re_graphloss:0.8251903057098389\n",
      "66 loss: 11.672194480895996, re_loss:0.010851352475583553,re_graphloss:0.8208419680595398\n",
      "67 loss: 11.306351661682129, re_loss:0.010490690357983112,re_graphloss:0.8156617283821106\n",
      "68 loss: 10.958745956420898, re_loss:0.010147017426788807,re_graphloss:0.8117281794548035\n",
      "69 loss: 10.626452445983887, re_loss:0.009817729704082012,re_graphloss:0.808722198009491\n",
      "70 loss: 10.311299324035645, re_loss:0.009504087269306183,re_graphloss:0.8072120547294617\n",
      "71 loss: 10.009136199951172, re_loss:0.009205871261656284,re_graphloss:0.8032650351524353\n",
      "72 loss: 9.722225189208984, re_loss:0.00892528798431158,re_graphloss:0.7969374656677246\n",
      "73 loss: 9.447319984436035, re_loss:0.008656107820570469,re_graphloss:0.7912118434906006\n",
      "74 loss: 9.185705184936523, re_loss:0.008396880701184273,re_graphloss:0.7888244390487671\n",
      "75 loss: 8.937206268310547, re_loss:0.008150333538651466,re_graphloss:0.7868729829788208\n",
      "76 loss: 8.700096130371094, re_loss:0.007916837930679321,re_graphloss:0.7832582592964172\n",
      "77 loss: 8.471759796142578, re_loss:0.007691850885748863,re_graphloss:0.779909610748291\n",
      "78 loss: 8.252739906311035, re_loss:0.007475561928004026,re_graphloss:0.777178168296814\n",
      "79 loss: 8.04507064819336, re_loss:0.007272018119692802,re_graphloss:0.7730528116226196\n",
      "80 loss: 7.845175743103027, re_loss:0.007077732123434544,re_graphloss:0.7674436569213867\n",
      "81 loss: 7.653657913208008, re_loss:0.006889665964990854,re_graphloss:0.7639915943145752\n",
      "82 loss: 7.4711151123046875, re_loss:0.006709538400173187,re_graphloss:0.7615764141082764\n",
      "83 loss: 7.296456336975098, re_loss:0.006539581809192896,re_graphloss:0.7568746209144592\n",
      "84 loss: 7.128048896789551, re_loss:0.0063761076889932156,re_graphloss:0.7519410252571106\n",
      "85 loss: 6.967216968536377, re_loss:0.006218757946044207,re_graphloss:0.7484588623046875\n",
      "86 loss: 6.815023899078369, re_loss:0.006068820133805275,re_graphloss:0.7462040185928345\n",
      "87 loss: 6.666968822479248, re_loss:0.005926281213760376,re_graphloss:0.7406876683235168\n",
      "88 loss: 6.525934219360352, re_loss:0.0057882340624928474,re_graphloss:0.7376999855041504\n",
      "89 loss: 6.391684532165527, re_loss:0.005653998348861933,re_graphloss:0.7376859784126282\n",
      "90 loss: 6.262078285217285, re_loss:0.005527992267161608,re_graphloss:0.7340860962867737\n",
      "91 loss: 6.136275291442871, re_loss:0.005407577380537987,re_graphloss:0.7286975979804993\n",
      "92 loss: 6.01657247543335, re_loss:0.005290894769132137,re_graphloss:0.7256776094436646\n",
      "93 loss: 5.901486873626709, re_loss:0.0051770685240626335,re_graphloss:0.7244182229042053\n",
      "94 loss: 5.789973735809326, re_loss:0.005069551523774862,re_graphloss:0.7204223871231079\n",
      "95 loss: 5.682937145233154, re_loss:0.004966878332197666,re_graphloss:0.716058611869812\n",
      "96 loss: 5.578726768493652, re_loss:0.004865707363933325,re_graphloss:0.7130193114280701\n",
      "97 loss: 5.479159832000732, re_loss:0.0047679743729531765,re_graphloss:0.7111853361129761\n",
      "98 loss: 5.382476806640625, re_loss:0.004673394374549389,re_graphloss:0.709082841873169\n",
      "99 loss: 5.2891364097595215, re_loss:0.0045846435241401196,re_graphloss:0.7044931650161743\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.sparse import dia_matrix, issparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "# torch.cuda.set_device(1)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, last_dim, n_num):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.n_num = n_num\n",
    "        self.fc1 = nn.Linear(n_num * last_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, n_num)\n",
    "        self.attention = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.T = 10\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        attention_sample = self.attention(x / self.T)\n",
    "        attention_view = torch.mean(attention_sample, dim=0, keepdim=True).squeeze()\n",
    "        return attention_view\n",
    "\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, last_dim, n_num=2):\n",
    "        super(FusionLayer, self).__init__()\n",
    "        self.n_num = n_num\n",
    "        self.attentionLayer = AttentionLayer(last_dim, n_num)\n",
    "\n",
    "    def forward(self, x, k):\n",
    "        y = torch.cat((x, k), 1)\n",
    "        weights = self.attentionLayer(y)\n",
    "        x_TMP = weights[0] * x + weights[1] * k\n",
    "        return x_TMP\n",
    "\n",
    "\n",
    "def distance(X, Y, square=True):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distances between two sets of samples\n",
    "    Basic framework: pytorch\n",
    "    :param X: d * n, where d is dimensions and n is number of data points in X\n",
    "    :param Y: d * m, where m is number of data points in Y\n",
    "    :param square: whether distances are squared, default value is True\n",
    "    :return: n * m, distance matrix\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    m = Y.shape[1]\n",
    "    x = torch.norm(X, dim=0)\n",
    "    x = x * x  # n * 1\n",
    "    x = torch.t(x.repeat(m, 1))\n",
    "\n",
    "    y = torch.norm(Y, dim=0)\n",
    "    y = y * y  # m * 1\n",
    "    y = y.repeat(n, 1)\n",
    "\n",
    "    crossing_term = torch.t(X).matmul(Y)\n",
    "    result = x + y - 2 * crossing_term\n",
    "    result = result.relu()\n",
    "    if not square:\n",
    "        result = torch.sqrt(result)\n",
    "    return result\n",
    "    \n",
    "def cal_weights_via_CAN(X, num_neighbors, links=0):\n",
    "    \"\"\"\n",
    "    Solve Problem: Clustering-with-Adaptive-Neighbors(CAN)\n",
    "    :param X: d * n\n",
    "    :param num_neighbors:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    size = X.shape[1]\n",
    "    distances = distance(X, X)\n",
    "    distances = torch.max(distances, torch.t(distances))\n",
    "    sorted_distances, _ = distances.sort(dim=1)\n",
    "    top_k = sorted_distances[:, num_neighbors + 1] # top_k = sorted_distances[:, num_neighbors]\n",
    "    top_k = torch.t(top_k.repeat(size, 1)) + 10**-10\n",
    "\n",
    "    sum_top_k = torch.sum(sorted_distances[:, 1:num_neighbors+1], dim=1)  # sum_top_k = torch.sum(sorted_distances[:, 0:num_neighbors], dim=1)\n",
    "    sum_top_k = torch.t(sum_top_k.repeat(size, 1))\n",
    "    sorted_distances = None\n",
    "    torch.cuda.empty_cache()\n",
    "    T = top_k - distances\n",
    "    distances = None\n",
    "    torch.cuda.empty_cache()\n",
    "    weights = torch.div(T, num_neighbors * top_k - sum_top_k)\n",
    "    T = None\n",
    "    top_k = None\n",
    "    sum_top_k = None\n",
    "    torch.cuda.empty_cache()\n",
    "    weights = weights.relu().cpu()\n",
    "    weights -= torch.diag(torch.diag(weights))\n",
    "    if links != 0:\n",
    "        links = torch.Tensor(links).cuda()\n",
    "        weights += torch.eye(size).cuda()\n",
    "        weights += links\n",
    "        weights /= weights.sum(dim=1).reshape([size, 1])\n",
    "    torch.cuda.empty_cache()\n",
    "    raw_weights = weights\n",
    "    weights = (weights + weights.t()) / 2\n",
    "    raw_weights = raw_weights.cuda()\n",
    "    weights = weights.cuda()\n",
    "    return weights, raw_weights\n",
    "\n",
    "def spatial_similarity(X,k):\n",
    "    \"\"\"\n",
    "    根据特征矩阵X构建k近邻空间矩阵\n",
    "    :param X: d x n 特征矩阵，d为维度，n为样本数量\n",
    "    :param k: 选择的近邻数量\n",
    "    :return: n x n 的矩阵\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # 计算所有样本间的平方距离矩阵（n x n）\n",
    "    distance_matrix = distance(X, X, square=True)\n",
    "    \n",
    "    # 对每行距离排序，获取排序后的索引（排除自身）\n",
    "    _, sorted_indices = torch.sort(distance_matrix, dim=1)  # sorted_indices形状为[n, n]\n",
    "    \n",
    "    # 提取每个样本的k近邻索引（跳过第0个自身）\n",
    "    knn_indices = sorted_indices[:, :k+1]  # 形状变为[n, k+1]\n",
    "    \n",
    "     # 创建全零矩阵\n",
    "    A = torch.zeros((n, n), dtype=torch.long).cuda()\n",
    "    \n",
    "    # 生成行索引矩阵\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n, k+1).cuda()\n",
    "    \n",
    "    # 计算绝对差值并填充\n",
    "    abs_diffs = torch.abs(row_indices - knn_indices)\n",
    "    A[row_indices, knn_indices] = abs_diffs\n",
    "\n",
    "    row_sums = A.sum(dim=1)\n",
    "    sigma = row_sums / (k+1)\n",
    "    \n",
    "    # 使用计算出来的sigma更新矩阵\n",
    "    spa_matrix = torch.exp(-A / (2 * sigma ** 2))\n",
    "    \n",
    "    return spa_matrix\n",
    "    \n",
    "def dot_product(z,k):\n",
    "    distances = distance(z.t(), z.t())\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    adj1 = softmax(-distances)\n",
    "    adj1 = adj1 * spatial_similarity(z.t(),k)\n",
    "    return adj1\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z):\n",
    "        super(AE, self).__init__()\n",
    "        # Encoder Layers\n",
    "        self.enc_1 = Linear(n_input, n_enc_1)\n",
    "        self.bn1 = nn.BatchNorm1d(n_enc_1)  # BatchNorm after Linear\n",
    "        self.enc_2 = Linear(n_enc_1, n_enc_2)\n",
    "        self.bn2 = nn.BatchNorm1d(n_enc_2)\n",
    "        self.enc_3 = Linear(n_enc_2, n_enc_3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_enc_3)\n",
    "        self.z_layer = Linear(n_enc_3, n_z)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.dec_1 = Linear(n_z, n_dec_1)\n",
    "        self.bn4 = nn.BatchNorm1d(n_dec_1)\n",
    "        self.dec_2 = Linear(n_dec_1, n_dec_2)\n",
    "        self.bn5 = nn.BatchNorm1d(n_dec_2)\n",
    "        self.dec_3 = Linear(n_dec_2, n_dec_3)\n",
    "        self.bn6 = nn.BatchNorm1d(n_dec_3)\n",
    "        self.x_bar_layer = Linear(n_dec_3, n_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: Linear + BatchNorm + ReLU\n",
    "        enc_h1 = F.relu(self.bn1(self.enc_1(x)))\n",
    "        enc_h2 = F.relu(self.bn2(self.enc_2(enc_h1)))\n",
    "        enc_h3 = F.relu(self.bn3(self.enc_3(enc_h2)))\n",
    "        z = self.z_layer(enc_h3)\n",
    "\n",
    "        # Decoder: Linear + BatchNorm + ReLU\n",
    "        dec_h1 = F.relu(self.bn4(self.dec_1(z)))\n",
    "        dec_h2 = F.relu(self.bn5(self.dec_2(dec_h1)))\n",
    "        dec_h3 = F.relu(self.bn6(self.dec_3(dec_h2)))\n",
    "        x_bar = self.x_bar_layer(dec_h3)\n",
    "\n",
    "        return x_bar, enc_h1, enc_h2, enc_h3, z, dec_h1, dec_h2, dec_h3\n",
    "\n",
    "\n",
    "class DTFU(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z, pretrain_path, k, num_bands):\n",
    "        super(DTFU, self).__init__()\n",
    "\n",
    "        # autoencoder for intra information\n",
    "        self.ael = AE(\n",
    "            n_enc_1=n_enc_1,\n",
    "            n_enc_2=n_enc_2,\n",
    "            n_enc_3=n_enc_3,\n",
    "            n_dec_1=n_dec_1,\n",
    "            n_dec_2=n_dec_2,\n",
    "            n_dec_3=n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "\n",
    "        # self.ael.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
    "\n",
    "        # GCN for inter information\n",
    "        self.gnn_1 = GNNLayer(n_input, n_enc_1)\n",
    "        self.gnn_2 = GNNLayer(n_enc_1, n_enc_2)\n",
    "        self.gnn_3 = GNNLayer(n_enc_2, n_enc_3)\n",
    "        self.gnn_4 = GNNLayer(n_enc_3, n_z)\n",
    "        self.gnn_5 = GNNLayer(n_z, n_dec_1)\n",
    "\n",
    "        self.gnn_7 = GNNLayer(n_dec_1, n_dec_2)\n",
    "        self.gnn_8 = GNNLayer(n_dec_2, n_dec_3)\n",
    "        self.gnn_9 = GNNLayer(n_dec_3, n_input)\n",
    "        \n",
    "        self.fuse1 = FusionLayer(n_enc_1)\n",
    "        self.fuse2 = FusionLayer(n_enc_2)\n",
    "        self.fuse3 = FusionLayer(n_enc_3)\n",
    "        self.fuse4 = FusionLayer(n_z)\n",
    "\n",
    "        self.fuse5 = FusionLayer(n_dec_1)\n",
    "        self.fuse6 = FusionLayer(n_dec_2)\n",
    "        self.fuse7 = FusionLayer(n_dec_3)\n",
    "        self.fuse8 = FusionLayer(n_input)\n",
    "\n",
    "        # degree\n",
    "        self.k = k\n",
    "        self.num_bands = num_bands\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # DNN Module\n",
    "        x_bar, tra1, tra2, tra3, z, dec_1, dec_2, dec_3 = self.ael(x)\n",
    "\n",
    "        sigma = 0.2\n",
    "\n",
    "        # GCN Module\n",
    "        h = self.gnn_1(x, adj)\n",
    "        h = self.fuse1(h, tra1)\n",
    "        adj1 = dot_product(h,self.k)\n",
    "        adj1= (1 - sigma) * adj1 + sigma * adj\n",
    "        adj1_f = adj1.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A1',{'a1':adj1_f})\n",
    "        adj1 = get_Laplacian_from_weights(adj1)   #归一化\n",
    "        \n",
    "        h = self.gnn_2(h, adj1)\n",
    "        h = self.fuse2(h, tra2)\n",
    "        adj2 = dot_product(h,self.k)\n",
    "        adj2= (1 - sigma) * adj2 + sigma * adj\n",
    "        adj2_f = adj2.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A2',{'a2':adj2_f})\n",
    "        adj2 = get_Laplacian_from_weights(adj2)\n",
    "\n",
    "        \n",
    "        h = self.gnn_3(h, adj2)\n",
    "        h = self.fuse3(h, tra3)\n",
    "        adj3 = dot_product(h,self.k)\n",
    "        adj3= (1 - sigma) * adj3 + sigma * adj\n",
    "        adj3_f = adj3.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A3',{'a3':adj3_f})\n",
    "        adj3 = get_Laplacian_from_weights(adj3)\n",
    "        \n",
    "\n",
    "        h = self.gnn_4(h, adj3)\n",
    "        h = self.fuse4(h, z)\n",
    "        adj4 = dot_product(h,self.k)\n",
    "        adj4= (1 - sigma) * adj4 + sigma * adj\n",
    "        adj4_f = adj4.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A4',{'a4':adj4_f})\n",
    "        adj4 = get_Laplacian_from_weights(adj4)\n",
    "\n",
    "\n",
    "        h1 = h\n",
    "        h = self.gnn_5(h, adj4)\n",
    "        h = self.fuse5(h, dec_1)\n",
    "        adj5 = dot_product(h,self.k)\n",
    "        adj5= (1 - sigma) * adj5 + sigma * adj\n",
    "        adj5_f = adj5.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A5',{'a5':adj5_f})\n",
    "        adj5 = get_Laplacian_from_weights(adj5)\n",
    "\n",
    "        h = self.gnn_7(h, adj5)\n",
    "        h = self.fuse6(h, dec_2)\n",
    "        adj6 = dot_product(h,self.k)\n",
    "        adj6= (1 - sigma) * adj6 + sigma * adj\n",
    "        adj6_f = adj6.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A6',{'a6':adj6_f})\n",
    "        adj6 = get_Laplacian_from_weights(adj6)\n",
    "        \n",
    "\n",
    "        h = self.gnn_8(h, adj6)\n",
    "        h = self.fuse7(h, dec_3)\n",
    "        adj7 = dot_product(h,self.k)\n",
    "        adj7= (1 - sigma) * adj7 + sigma * adj\n",
    "        adj7_f = adj7.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A7',{'a7':adj7_f})\n",
    "        adj7 = get_Laplacian_from_weights(adj7)\n",
    "\n",
    "        h = self.gnn_9(h, adj7)\n",
    "        h = self.fuse8(h, x_bar)\n",
    "        A_pred = dot_product(h,self.k)\n",
    "        A_pred = (1 - sigma) * A_pred + sigma * adj\n",
    "\n",
    "        return x_bar, z, A_pred, h, h1\n",
    "\n",
    "\n",
    "\n",
    "def get_Laplacian_from_weights(weights):\n",
    "    degree = torch.sum(weights, dim=1).pow(-0.5)\n",
    "    return (weights * degree).t()*degree\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"\n",
    "    Checks if any of the gradients are NaN, Inf\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            # Check if the gradients contain NaN or Inf\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradient for parameter {param}\")\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(f\"Inf detected in gradient for parameter {param}\")\n",
    "\n",
    "\n",
    "\n",
    "def train_dtfu(dataset, n_input, n_z, lr, name, k, epoches, pretrain_path, lamb_da):\n",
    "    num = dataset.x.shape[0]\n",
    "    model = DTFU(512, 256, 128, 128, 256, 512,\n",
    "                 n_input=n_input,\n",
    "                 n_z=n_z,\n",
    "                 pretrain_path = pretrain_path, k=k, num_bands = num).to(device)\n",
    "    print(model)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # KNN Graph\n",
    "    data = torch.Tensor(dataset.x).to(device)\n",
    "    weights, raw_weights = cal_weights_via_CAN(data.t(), k)\n",
    "    \n",
    "    weights_spectral = weights.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    spatial_matrix = spatial_similarity(data.t(),k)\n",
    "    weights_spatial = spatial_matrix.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    weights = weights * spatial_matrix\n",
    "    weights_f = weights.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    L = get_Laplacian_from_weights(weights)\n",
    "    L_f = L.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    print(\"completed!\")\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epoches):\n",
    "\n",
    "        x_bar,z, A_pred, Z, Z_ = model(data,L)\n",
    "\n",
    "        re_loss = F.mse_loss(x_bar, data)\n",
    "        re_graphloss = torch.sum(weights * torch.log(weights / (A_pred+10**-10) + 10**-10))\n",
    "        re_graphloss = re_graphloss / num\n",
    "\n",
    "        loss = lamb_da[1] * re_loss + lamb_da[0] * re_graphloss \n",
    "        losses.append(loss.item())\n",
    "        print('{} loss: {}, re_loss:{},re_graphloss:{}'.format(epoch, loss.item(),re_loss.item(),re_graphloss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        check_gradients(model)\n",
    "        optimizer.step()\n",
    "    return A_pred, Z_, Z, losses\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # name = 'DC191'\n",
    "    name = 'IP220'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Original_Img = sio.loadmat('data/DC_Sub.mat')['DC_Sub']\n",
    "    Original_Img = sio.loadmat('data/Indian_pines.mat')['indian_pines']\n",
    "    new_l = []\n",
    "    c = 0\n",
    "    for k in range(Original_Img.shape[2]):\n",
    "        l = []\n",
    "        for i in range(Original_Img.shape[0]):\n",
    "            for j in range(Original_Img.shape[1]):\n",
    "                l.append(Original_Img[i][j][k])\n",
    "                c = c + 1\n",
    "        new_l.append(l)\n",
    "    X = np.array(new_l)\n",
    "    \n",
    "    print('X.shape=', X.shape)\n",
    "    # X归一化\n",
    "    X_min = X.min(axis=0)  # 对每一列计算最小值\n",
    "    X_max = X.max(axis=0)  # 对每一列计算最大值\n",
    "    X = (X - X_min) / (X_max - X_min)  # 对每一列分别进行归一化\n",
    "    \n",
    "    # y = sio.loadmat('data/DC_Sub.mat')['grd']\n",
    "    y = sio.loadmat('data/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    y = y.reshape(X.shape[1])\n",
    "    \n",
    "    dataset = load_data(X,y)\n",
    "    pretrain_path = 'model/{}.pkl'.format(name)\n",
    "    k = 10\n",
    "    lr = 1e-4\n",
    "    n_input = X.shape[1]\n",
    "    n_z = 64\n",
    "    epoches = 100\n",
    "    \n",
    "    lamb_da = [1.0, 1000]# re_graphloss,re_loss\n",
    "    A_pred, Z_, Z, losses = train_dtfu(dataset, n_input, n_z, lr, name, k, epoches, pretrain_path, lamb_da)\n",
    "    plt.plot(range(1, epoches + 1), losses, marker='o', linestyle='-', color='b', label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6636cb84-40ad-459b-86dd-4aaa6291c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 21025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:5,[15, 53, 97, 120, 205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:10,[9, 28, 39, 52, 68, 94, 99, 122, 141, 205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:15,[3, 5, 8, 14, 23, 28, 39, 52, 67, 74, 94, 99, 101, 118, 122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:20,[3, 8, 12, 22, 28, 32, 39, 52, 53, 67, 74, 75, 94, 99, 101, 119, 122, 143, 173, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:25,[3, 8, 12, 17, 19, 23, 28, 32, 37, 39, 52, 62, 63, 66, 73, 75, 87, 98, 99, 101, 119, 122, 143, 173, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:30,[3, 8, 11, 17, 19, 23, 28, 32, 36, 37, 38, 41, 52, 54, 60, 62, 64, 67, 73, 75, 87, 93, 99, 101, 118, 122, 125, 143, 173, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:35,[3, 5, 8, 12, 17, 19, 22, 26, 32, 33, 36, 38, 41, 52, 53, 54, 60, 61, 62, 64, 67, 72, 75, 78, 87, 93, 99, 101, 118, 119, 120, 122, 144, 173, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:40,[3, 5, 8, 12, 17, 18, 23, 28, 32, 33, 34, 36, 37, 39, 40, 41, 47, 52, 54, 56, 58, 60, 61, 64, 67, 72, 74, 75, 78, 87, 93, 100, 101, 118, 119, 120, 122, 144, 174, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:45,[3, 5, 6, 8, 12, 17, 18, 23, 28, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 46, 52, 53, 54, 56, 57, 58, 60, 64, 67, 72, 74, 75, 78, 87, 93, 99, 100, 101, 118, 119, 120, 122, 144, 173, 215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:50,[3, 5, 6, 8, 12, 17, 18, 21, 24, 28, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 47, 52, 53, 54, 56, 58, 60, 61, 63, 64, 65, 70, 72, 74, 75, 76, 78, 90, 93, 94, 99, 100, 101, 118, 119, 120, 122, 144, 173, 215]\n"
     ]
    }
   ],
   "source": [
    "# import scipy.io as sio\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import scipy.io as sio\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.stats import wasserstein_distance\n",
    "from joblib import Parallel, delayed  # 用于并行化\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# 计算 SSIM\n",
    "def calculate_ssim(X):\n",
    "    num_bands = X.shape[0]\n",
    "    ssim_matrix = np.zeros((num_bands, num_bands))\n",
    "\n",
    "    def compute_ssim(i, j):\n",
    "        return ssim(X[i], X[j], data_range=1)\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_ssim)(i, j) for i in range(num_bands) for j in range(i+1, num_bands))\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(num_bands):\n",
    "        for j in range(i+1, num_bands):\n",
    "            ssim_matrix[i, j] = results[idx]\n",
    "            ssim_matrix[j, i] = results[idx]  # 对称填充下三角矩阵\n",
    "            idx += 1\n",
    "            \n",
    "    return ssim_matrix\n",
    "\n",
    "# 计算 IcSDD\n",
    "def calculate_icSDD(X, grp, ssim_matrix):\n",
    "    num_bands = X.shape[0]\n",
    "    icSDDs = np.zeros(num_bands)\n",
    "\n",
    "    for i in range(num_bands):\n",
    "        same_cluster = grp == grp[i]  # 同簇波段\n",
    "        different_cluster = grp != grp[i]  # 不同簇波段\n",
    "\n",
    "        positive_similarities = ssim_matrix[i, same_cluster]\n",
    "        negative_similarities = ssim_matrix[i, different_cluster]\n",
    "\n",
    "        # 计算 IcSDD\n",
    "        icSDD = wasserstein_distance(positive_similarities, negative_similarities)\n",
    "        icSDDs[i] = icSDD\n",
    "\n",
    "    return icSDDs\n",
    "\n",
    "# 计算特征矩阵的熵\n",
    "def Entrop(X):\n",
    "    G = 256\n",
    "    L, N = X.shape\n",
    "    rak_val = np.zeros(L)\n",
    "    minX = np.min(X)\n",
    "    maxX = np.max(X)\n",
    "    edge = np.linspace(minX, maxX, G)\n",
    "    for i in range(L):\n",
    "        histX, _ = np.histogram(X[i, :], bins=edge, density=True)\n",
    "        rak_val[i] = -np.sum(histX * np.log2(histX + np.finfo(float).eps))\n",
    "    return rak_val\n",
    "\n",
    "# 计算标准拉普拉斯矩阵\n",
    "def compute_laplacian(W):\n",
    "    D = np.diag(W.sum(axis=1))  # 度矩阵，D[i, i] 是第 i 个节点的度数\n",
    "    L = D - W  # 标准拉普拉斯矩阵\n",
    "    return L\n",
    "\n",
    "# 特征值分解并选择 k 个最小特征值对应的特征向量\n",
    "def compute_eigenvectors(L, k):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    return eigenvectors[:, :k]\n",
    "\n",
    "# 使用 K-means 聚类\n",
    "def perform_kmeans(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "    return kmeans.fit_predict(X)\n",
    "\n",
    "# 主函数\n",
    "def spectral_clustering(W, n_clusters):\n",
    "    L = compute_laplacian(W)\n",
    "    k = n_clusters  # 聚类数就是选取的特征向量个数\n",
    "    eigenvectors = compute_eigenvectors(L, k)\n",
    "    labels = perform_kmeans(eigenvectors, n_clusters)\n",
    "    return labels\n",
    "\n",
    "# 数据预处理\n",
    "# Original_Img = sio.loadmat('data/DC_Sub.mat')['DC_Sub']\n",
    "Original_Img = sio.loadmat('data/Indian_pines.mat')['indian_pines']\n",
    "\n",
    "new_l = []\n",
    "c = 0\n",
    "for k in range(Original_Img.shape[2]):\n",
    "    l = []\n",
    "    for i in range(Original_Img.shape[0]):\n",
    "        for j in range(Original_Img.shape[1]):\n",
    "            l.append(Original_Img[i][j][k])\n",
    "            c = c + 1\n",
    "    new_l.append(l)\n",
    "\n",
    "X = np.array(new_l)\n",
    "print(X.shape)\n",
    "\n",
    "# 对每一列进行归一化\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "W = sio.loadmat('A_pred')['A']\n",
    "W = (W + W.T) / 2\n",
    "\n",
    "\n",
    "with open(\"selected_bands.txt\", \"w\") as file:\n",
    "    for num_bands in [5,10,15,20,25,30,35,40,45,50]:\n",
    "    \n",
    "        grp = spectral_clustering(W, num_bands)\n",
    "        \n",
    "        # 计算 SSIM\n",
    "        ssim_matrix = calculate_ssim(X)\n",
    "        \n",
    "        # 计算 IcSDD 和熵\n",
    "        icSDDs = calculate_icSDD(X, grp, ssim_matrix)\n",
    "        entropies = Entrop(X)\n",
    "        \n",
    "        # 归一化 IcSDD 和熵\n",
    "        icSDDs = (icSDDs - icSDDs.min()) / (icSDDs.max() - icSDDs.min())\n",
    "        entropies = (entropies - entropies.min()) / (entropies.max() - entropies.min())\n",
    "        \n",
    "        # 合并值\n",
    "        values = icSDDs + entropies\n",
    "        \n",
    "        # 获取每个类别的最大值及其索引\n",
    "        max_values = {}\n",
    "        for i, label in enumerate(grp):\n",
    "            if label not in max_values or values[i] > max_values[label][0]:\n",
    "                max_values[label] = (values[i], i)\n",
    "        \n",
    "        # 存储每个类别最大值样本的索引\n",
    "        selected_bands = [index for _, index in max_values.values()]\n",
    "        selected_bands.sort()\n",
    "        selected_bands = [x+1 for x in selected_bands]\n",
    "    \n",
    "        print(f\"num_bands:{num_bands},{selected_bands}\")\n",
    "        file.write(f\"num_bands:{num_bands}, {selected_bands}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cd878-2bf3-4c2a-bdda-b6d64a443210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python GAE",
   "language": "python",
   "name": "gae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
