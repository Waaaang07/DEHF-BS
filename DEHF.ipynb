{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9651985-97a9-43ec-813f-250fd1d04e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.init as init\n",
    "\n",
    "class GNNLayer(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        # init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, features, adj, active=True):\n",
    "        support = torch.mm(features, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if active:\n",
    "            output = F.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4518adb5-64e3-4f5b-8380-8965b143f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import pairwise_distances as pair\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class load_data(Dataset):\n",
    "    def __init__(self, dataset, label):\n",
    "        self.x = dataset\n",
    "        self.y = label\n",
    "        # data_mat.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(np.array(self.x[idx])), \\\n",
    "            torch.from_numpy(np.array(self.y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4016d37e-b054-4aaa-9cdd-5775c4164f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (220, 21025)\n",
      "DTFU(\n",
      "  (ael): AE(\n",
      "    (enc_1): Linear(in_features=21025, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc_3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (z_layer): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dec_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec_3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (x_bar_layer): Linear(in_features=512, out_features=21025, bias=True)\n",
      "  )\n",
      "  (gnn_1): GNNLayer()\n",
      "  (gnn_2): GNNLayer()\n",
      "  (gnn_3): GNNLayer()\n",
      "  (gnn_4): GNNLayer()\n",
      "  (gnn_5): GNNLayer()\n",
      "  (gnn_7): GNNLayer()\n",
      "  (gnn_8): GNNLayer()\n",
      "  (gnn_9): GNNLayer()\n",
      "  (fuse1): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse2): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse3): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=256, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse4): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=128, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse5): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=256, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse6): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse7): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fuse8): FusionLayer(\n",
      "    (attentionLayer): AttentionLayer(\n",
      "      (fc1): Linear(in_features=42050, out_features=500, bias=True)\n",
      "      (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      "      (attention): Softmax(dim=1)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "completed!\n",
      "0 loss: 351.02874755859375, re_loss:0.3496732711791992,re_graphloss:1.3554836511611938\n",
      "1 loss: 333.9752502441406, re_loss:0.33274462819099426,re_graphloss:1.2306251525878906\n",
      "2 loss: 315.9450378417969, re_loss:0.31483763456344604,re_graphloss:1.107391357421875\n",
      "3 loss: 302.8060607910156, re_loss:0.3016539216041565,re_graphloss:1.152133822441101\n",
      "4 loss: 286.7848205566406, re_loss:0.2856789231300354,re_graphloss:1.1058980226516724\n",
      "5 loss: 271.118408203125, re_loss:0.27006885409355164,re_graphloss:1.0495710372924805\n",
      "6 loss: 255.98983764648438, re_loss:0.25496044754981995,re_graphloss:1.0293829441070557\n",
      "7 loss: 242.2930450439453, re_loss:0.2412508726119995,re_graphloss:1.0421745777130127\n",
      "8 loss: 227.73191833496094, re_loss:0.22669197618961334,re_graphloss:1.0399419069290161\n",
      "9 loss: 213.916748046875, re_loss:0.21290166676044464,re_graphloss:1.0150701999664307\n",
      "10 loss: 200.45533752441406, re_loss:0.1994544416666031,re_graphloss:1.0008952617645264\n",
      "11 loss: 189.24180603027344, re_loss:0.18826548755168915,re_graphloss:0.9763240218162537\n",
      "12 loss: 178.0194854736328, re_loss:0.17705704271793365,re_graphloss:0.9624543786048889\n",
      "13 loss: 167.00180053710938, re_loss:0.166062593460083,re_graphloss:0.939208984375\n",
      "14 loss: 156.4959259033203, re_loss:0.15556448698043823,re_graphloss:0.9314469695091248\n",
      "15 loss: 146.7487030029297, re_loss:0.14580948650836945,re_graphloss:0.939202070236206\n",
      "16 loss: 137.25021362304688, re_loss:0.13631603121757507,re_graphloss:0.934195876121521\n",
      "17 loss: 128.56109619140625, re_loss:0.12762869894504547,re_graphloss:0.9323983788490295\n",
      "18 loss: 120.32293701171875, re_loss:0.11939574033021927,re_graphloss:0.9271982312202454\n",
      "19 loss: 112.57231140136719, re_loss:0.11164586246013641,re_graphloss:0.9264506697654724\n",
      "20 loss: 105.50545501708984, re_loss:0.10460373759269714,re_graphloss:0.9017190933227539\n",
      "21 loss: 98.5933837890625, re_loss:0.09766773879528046,re_graphloss:0.9256427884101868\n",
      "22 loss: 92.24158477783203, re_loss:0.09131219238042831,re_graphloss:0.929388165473938\n",
      "23 loss: 86.2137451171875, re_loss:0.08528375625610352,re_graphloss:0.9299920201301575\n",
      "24 loss: 80.77596282958984, re_loss:0.07984694838523865,re_graphloss:0.9290164709091187\n",
      "25 loss: 75.703369140625, re_loss:0.07477258890867233,re_graphloss:0.9307786226272583\n",
      "26 loss: 70.94929504394531, re_loss:0.07001569867134094,re_graphloss:0.9335966110229492\n",
      "27 loss: 66.5228271484375, re_loss:0.06558651477098465,re_graphloss:0.9363128542900085\n",
      "28 loss: 62.35767364501953, re_loss:0.06142351031303406,re_graphloss:0.9341607093811035\n",
      "29 loss: 58.576446533203125, re_loss:0.057645976543426514,re_graphloss:0.9304701089859009\n",
      "30 loss: 55.07949447631836, re_loss:0.05415321886539459,re_graphloss:0.9262765645980835\n",
      "31 loss: 51.807804107666016, re_loss:0.05088592320680618,re_graphloss:0.9218811988830566\n",
      "32 loss: 48.76020431518555, re_loss:0.04784452170133591,re_graphloss:0.9156850576400757\n",
      "33 loss: 45.95168685913086, re_loss:0.04503805935382843,re_graphloss:0.9136288166046143\n",
      "34 loss: 43.35955047607422, re_loss:0.04244726523756981,re_graphloss:0.9122860431671143\n",
      "35 loss: 40.904354095458984, re_loss:0.03999129682779312,re_graphloss:0.9130557179450989\n",
      "36 loss: 38.62675857543945, re_loss:0.03770967200398445,re_graphloss:0.9170868992805481\n",
      "37 loss: 36.53128433227539, re_loss:0.03561408072710037,re_graphloss:0.9172027111053467\n",
      "38 loss: 34.566932678222656, re_loss:0.03365553170442581,re_graphloss:0.911398708820343\n",
      "39 loss: 32.747337341308594, re_loss:0.03184325248003006,re_graphloss:0.9040852785110474\n",
      "40 loss: 31.047151565551758, re_loss:0.03014986962080002,re_graphloss:0.8972809910774231\n",
      "41 loss: 29.448373794555664, re_loss:0.028557905927300453,re_graphloss:0.8904691934585571\n",
      "42 loss: 27.987905502319336, re_loss:0.02709989808499813,re_graphloss:0.8880075216293335\n",
      "43 loss: 26.59556770324707, re_loss:0.025707630440592766,re_graphloss:0.8879379630088806\n",
      "44 loss: 25.301082611083984, re_loss:0.024414610117673874,re_graphloss:0.8864725828170776\n",
      "45 loss: 24.07878875732422, re_loss:0.02319657988846302,re_graphloss:0.8822105526924133\n",
      "46 loss: 22.940893173217773, re_loss:0.022060519084334373,re_graphloss:0.8803751468658447\n",
      "47 loss: 21.8769588470459, re_loss:0.02100101299583912,re_graphloss:0.8759465217590332\n",
      "48 loss: 20.86441993713379, re_loss:0.01999012380838394,re_graphloss:0.874295711517334\n",
      "49 loss: 19.928377151489258, re_loss:0.0190512053668499,re_graphloss:0.8771729469299316\n",
      "50 loss: 19.049747467041016, re_loss:0.01816973090171814,re_graphloss:0.8800169825553894\n",
      "51 loss: 18.229272842407227, re_loss:0.01734868250787258,re_graphloss:0.8805907368659973\n",
      "52 loss: 17.454845428466797, re_loss:0.016576938331127167,re_graphloss:0.8779062032699585\n",
      "53 loss: 16.72532844543457, re_loss:0.0158514603972435,re_graphloss:0.8738688826560974\n",
      "54 loss: 16.0346622467041, re_loss:0.015167451463639736,re_graphloss:0.8672108054161072\n",
      "55 loss: 15.395110130310059, re_loss:0.014534943737089634,re_graphloss:0.8601662516593933\n",
      "56 loss: 14.791373252868652, re_loss:0.013935375958681107,re_graphloss:0.8559967875480652\n",
      "57 loss: 14.217931747436523, re_loss:0.013367747887969017,re_graphloss:0.8501834869384766\n",
      "58 loss: 13.683418273925781, re_loss:0.012839466333389282,re_graphloss:0.8439520597457886\n",
      "59 loss: 13.174415588378906, re_loss:0.012337999418377876,re_graphloss:0.8364163041114807\n",
      "60 loss: 12.695382118225098, re_loss:0.011863484047353268,re_graphloss:0.8318973779678345\n",
      "61 loss: 12.23637580871582, re_loss:0.011411131359636784,re_graphloss:0.825244665145874\n",
      "62 loss: 11.810694694519043, re_loss:0.010992036201059818,re_graphloss:0.8186588287353516\n",
      "63 loss: 11.402976036071777, re_loss:0.01058723870664835,re_graphloss:0.8157377243041992\n",
      "64 loss: 11.022143363952637, re_loss:0.010207738727331161,re_graphloss:0.8144046664237976\n",
      "65 loss: 10.65872859954834, re_loss:0.009846366010606289,re_graphloss:0.8123630881309509\n",
      "66 loss: 10.316177368164062, re_loss:0.009506084024906158,re_graphloss:0.8100934028625488\n",
      "67 loss: 9.992752075195312, re_loss:0.009185532107949257,re_graphloss:0.8072194457054138\n",
      "68 loss: 9.682361602783203, re_loss:0.00887861754745245,re_graphloss:0.8037441968917847\n",
      "69 loss: 9.387194633483887, re_loss:0.008588125929236412,re_graphloss:0.7990680932998657\n",
      "70 loss: 9.107038497924805, re_loss:0.008311308920383453,re_graphloss:0.7957297563552856\n",
      "71 loss: 8.837854385375977, re_loss:0.008046197704970837,re_graphloss:0.7916561961174011\n",
      "72 loss: 8.584086418151855, re_loss:0.007796348072588444,re_graphloss:0.7877379655838013\n",
      "73 loss: 8.343059539794922, re_loss:0.0075585199519991875,re_graphloss:0.7845397591590881\n",
      "74 loss: 8.111082077026367, re_loss:0.0073309424333274364,re_graphloss:0.7801399230957031\n",
      "75 loss: 7.890192985534668, re_loss:0.007115129381418228,re_graphloss:0.7750636339187622\n",
      "76 loss: 7.679747581481934, re_loss:0.006909658201038837,re_graphloss:0.7700890302658081\n",
      "77 loss: 7.478726863861084, re_loss:0.006713699083775282,re_graphloss:0.7650280594825745\n",
      "78 loss: 7.286025524139404, re_loss:0.006526713725179434,re_graphloss:0.7593117356300354\n",
      "79 loss: 7.104111194610596, re_loss:0.006348586641252041,re_graphloss:0.7555246353149414\n",
      "80 loss: 6.929508209228516, re_loss:0.006177973933517933,re_graphloss:0.7515343427658081\n",
      "81 loss: 6.760769844055176, re_loss:0.006015012506395578,re_graphloss:0.745756983757019\n",
      "82 loss: 6.598738193511963, re_loss:0.005859930068254471,re_graphloss:0.7388080954551697\n",
      "83 loss: 6.445557594299316, re_loss:0.005712267477065325,re_graphloss:0.7332899570465088\n",
      "84 loss: 6.298956871032715, re_loss:0.005569536238908768,re_graphloss:0.7294208407402039\n",
      "85 loss: 6.15931510925293, re_loss:0.005434462800621986,re_graphloss:0.7248523235321045\n",
      "86 loss: 6.024645805358887, re_loss:0.005304570775479078,re_graphloss:0.7200751304626465\n",
      "87 loss: 5.894575119018555, re_loss:0.005180856212973595,re_graphloss:0.7137189507484436\n",
      "88 loss: 5.771342754364014, re_loss:0.005062428303062916,re_graphloss:0.7089142799377441\n",
      "89 loss: 5.65138053894043, re_loss:0.004947416950017214,re_graphloss:0.7039639353752136\n",
      "90 loss: 5.537213325500488, re_loss:0.00483753951266408,re_graphloss:0.699673593044281\n",
      "91 loss: 5.4282546043396, re_loss:0.004731364548206329,re_graphloss:0.6968898177146912\n",
      "92 loss: 5.321564674377441, re_loss:0.0046308632008731365,re_graphloss:0.6907015442848206\n",
      "93 loss: 5.219621658325195, re_loss:0.0045314054004848,re_graphloss:0.6882162690162659\n",
      "94 loss: 5.119936943054199, re_loss:0.004436662420630455,re_graphloss:0.6832749843597412\n",
      "95 loss: 5.023921966552734, re_loss:0.00434268219396472,re_graphloss:0.6812395453453064\n",
      "96 loss: 4.930952548980713, re_loss:0.004253869876265526,re_graphloss:0.6770826578140259\n",
      "97 loss: 4.8422956466674805, re_loss:0.004169115796685219,re_graphloss:0.6731798648834229\n",
      "98 loss: 4.75655460357666, re_loss:0.00408786628395319,re_graphloss:0.6686882376670837\n",
      "99 loss: 4.67441463470459, re_loss:0.0040100207552313805,re_graphloss:0.6643939018249512\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.sparse import dia_matrix, issparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "# torch.cuda.set_device(1)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, last_dim, n_num):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.n_num = n_num\n",
    "        self.fc1 = nn.Linear(n_num * last_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, n_num)\n",
    "        self.attention = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.T = 10\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        attention_sample = self.attention(x / self.T)\n",
    "        attention_view = torch.mean(attention_sample, dim=0, keepdim=True).squeeze()\n",
    "        return attention_view\n",
    "\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, last_dim, n_num=2):\n",
    "        super(FusionLayer, self).__init__()\n",
    "        self.n_num = n_num\n",
    "        self.attentionLayer = AttentionLayer(last_dim, n_num)\n",
    "\n",
    "    def forward(self, x, k):\n",
    "        y = torch.cat((x, k), 1)\n",
    "        weights = self.attentionLayer(y)\n",
    "        x_TMP = weights[0] * x + weights[1] * k\n",
    "        return x_TMP\n",
    "\n",
    "\n",
    "def distance(X, Y, square=True):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distances between two sets of samples\n",
    "    Basic framework: pytorch\n",
    "    :param X: d * n, where d is dimensions and n is number of data points in X\n",
    "    :param Y: d * m, where m is number of data points in Y\n",
    "    :param square: whether distances are squared, default value is True\n",
    "    :return: n * m, distance matrix\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    m = Y.shape[1]\n",
    "    x = torch.norm(X, dim=0)\n",
    "    x = x * x  # n * 1\n",
    "    x = torch.t(x.repeat(m, 1))\n",
    "\n",
    "    y = torch.norm(Y, dim=0)\n",
    "    y = y * y  # m * 1\n",
    "    y = y.repeat(n, 1)\n",
    "\n",
    "    crossing_term = torch.t(X).matmul(Y)\n",
    "    result = x + y - 2 * crossing_term\n",
    "    result = result.relu()\n",
    "    if not square:\n",
    "        result = torch.sqrt(result)\n",
    "    return result\n",
    "    \n",
    "def cal_weights_via_CAN(X, num_neighbors, links=0):\n",
    "    \"\"\"\n",
    "    Solve Problem: Clustering-with-Adaptive-Neighbors(CAN)\n",
    "    :param X: d * n\n",
    "    :param num_neighbors:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    size = X.shape[1]\n",
    "    distances = distance(X, X)\n",
    "    distances = torch.max(distances, torch.t(distances))\n",
    "    sorted_distances, _ = distances.sort(dim=1)\n",
    "    top_k = sorted_distances[:, num_neighbors + 1] # top_k = sorted_distances[:, num_neighbors]\n",
    "    top_k = torch.t(top_k.repeat(size, 1)) + 10**-10\n",
    "\n",
    "    sum_top_k = torch.sum(sorted_distances[:, 1:num_neighbors+1], dim=1)  # sum_top_k = torch.sum(sorted_distances[:, 0:num_neighbors], dim=1)\n",
    "    sum_top_k = torch.t(sum_top_k.repeat(size, 1))\n",
    "    sorted_distances = None\n",
    "    torch.cuda.empty_cache()\n",
    "    T = top_k - distances\n",
    "    distances = None\n",
    "    torch.cuda.empty_cache()\n",
    "    weights = torch.div(T, num_neighbors * top_k - sum_top_k)\n",
    "    T = None\n",
    "    top_k = None\n",
    "    sum_top_k = None\n",
    "    torch.cuda.empty_cache()\n",
    "    weights = weights.relu().cpu()\n",
    "    weights -= torch.diag(torch.diag(weights))\n",
    "    if links != 0:\n",
    "        links = torch.Tensor(links).cuda()\n",
    "        weights += torch.eye(size).cuda()\n",
    "        weights += links\n",
    "        weights /= weights.sum(dim=1).reshape([size, 1])\n",
    "    torch.cuda.empty_cache()\n",
    "    raw_weights = weights\n",
    "    weights = (weights + weights.t()) / 2\n",
    "    raw_weights = raw_weights.cuda()\n",
    "    weights = weights.cuda()\n",
    "    return weights, raw_weights\n",
    "\n",
    "def spatial_similarity(X,k):\n",
    "    \"\"\"\n",
    "    根据特征矩阵X构建k近邻空间矩阵\n",
    "    :param X: d x n 特征矩阵，d为维度，n为样本数量\n",
    "    :param k: 选择的近邻数量\n",
    "    :return: n x n 的矩阵\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # 计算所有样本间的平方距离矩阵（n x n）\n",
    "    distance_matrix = distance(X, X, square=True)\n",
    "    \n",
    "    # 对每行距离排序，获取排序后的索引（排除自身）\n",
    "    _, sorted_indices = torch.sort(distance_matrix, dim=1)  # sorted_indices形状为[n, n]\n",
    "    \n",
    "    # 提取每个样本的k近邻索引（跳过第0个自身）\n",
    "    knn_indices = sorted_indices[:, :k+1]  # 形状变为[n, k+1]\n",
    "    \n",
    "     # 创建全零矩阵\n",
    "    A = torch.zeros((n, n), dtype=torch.long).cuda()\n",
    "    \n",
    "    # 生成行索引矩阵\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n, k+1).cuda()\n",
    "    \n",
    "    # 计算绝对差值并填充\n",
    "    abs_diffs = torch.abs(row_indices - knn_indices)\n",
    "    A[row_indices, knn_indices] = abs_diffs\n",
    "\n",
    "    row_sums = A.sum(dim=1)\n",
    "    sigma = row_sums / (k+1)\n",
    "    \n",
    "    # 使用计算出来的sigma更新矩阵\n",
    "    spa_matrix = torch.exp(-A / (2 * sigma ** 2))\n",
    "    \n",
    "    return spa_matrix\n",
    "    \n",
    "def dot_product(z,k):\n",
    "    distances = distance(z.t(), z.t())\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    adj1 = softmax(-distances)\n",
    "    adj1 = adj1 * spatial_similarity(z.t(),k)\n",
    "    return adj1\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z):\n",
    "        super(AE, self).__init__()\n",
    "        # Encoder Layers\n",
    "        self.enc_1 = Linear(n_input, n_enc_1)\n",
    "        self.bn1 = nn.BatchNorm1d(n_enc_1)  # BatchNorm after Linear\n",
    "        self.enc_2 = Linear(n_enc_1, n_enc_2)\n",
    "        self.bn2 = nn.BatchNorm1d(n_enc_2)\n",
    "        self.enc_3 = Linear(n_enc_2, n_enc_3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_enc_3)\n",
    "        self.z_layer = Linear(n_enc_3, n_z)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.dec_1 = Linear(n_z, n_dec_1)\n",
    "        self.bn4 = nn.BatchNorm1d(n_dec_1)\n",
    "        self.dec_2 = Linear(n_dec_1, n_dec_2)\n",
    "        self.bn5 = nn.BatchNorm1d(n_dec_2)\n",
    "        self.dec_3 = Linear(n_dec_2, n_dec_3)\n",
    "        self.bn6 = nn.BatchNorm1d(n_dec_3)\n",
    "        self.x_bar_layer = Linear(n_dec_3, n_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: Linear + BatchNorm + ReLU\n",
    "        enc_h1 = F.relu(self.bn1(self.enc_1(x)))\n",
    "        enc_h2 = F.relu(self.bn2(self.enc_2(enc_h1)))\n",
    "        enc_h3 = F.relu(self.bn3(self.enc_3(enc_h2)))\n",
    "        z = self.z_layer(enc_h3)\n",
    "\n",
    "        # Decoder: Linear + BatchNorm + ReLU\n",
    "        dec_h1 = F.relu(self.bn4(self.dec_1(z)))\n",
    "        dec_h2 = F.relu(self.bn5(self.dec_2(dec_h1)))\n",
    "        dec_h3 = F.relu(self.bn6(self.dec_3(dec_h2)))\n",
    "        x_bar = self.x_bar_layer(dec_h3)\n",
    "\n",
    "        return x_bar, enc_h1, enc_h2, enc_h3, z, dec_h1, dec_h2, dec_h3\n",
    "\n",
    "\n",
    "class DTFU(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z, pretrain_path, k, num_bands):\n",
    "        super(DTFU, self).__init__()\n",
    "\n",
    "        # autoencoder for intra information\n",
    "        self.ael = AE(\n",
    "            n_enc_1=n_enc_1,\n",
    "            n_enc_2=n_enc_2,\n",
    "            n_enc_3=n_enc_3,\n",
    "            n_dec_1=n_dec_1,\n",
    "            n_dec_2=n_dec_2,\n",
    "            n_dec_3=n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "\n",
    "        # self.ael.load_state_dict(torch.load(pretrain_path, map_location='cpu'))\n",
    "\n",
    "        # GCN for inter information\n",
    "        self.gnn_1 = GNNLayer(n_input, n_enc_1)\n",
    "        self.gnn_2 = GNNLayer(n_enc_1, n_enc_2)\n",
    "        self.gnn_3 = GNNLayer(n_enc_2, n_enc_3)\n",
    "        self.gnn_4 = GNNLayer(n_enc_3, n_z)\n",
    "        self.gnn_5 = GNNLayer(n_z, n_dec_1)\n",
    "\n",
    "        self.gnn_7 = GNNLayer(n_dec_1, n_dec_2)\n",
    "        self.gnn_8 = GNNLayer(n_dec_2, n_dec_3)\n",
    "        self.gnn_9 = GNNLayer(n_dec_3, n_input)\n",
    "        \n",
    "        self.fuse1 = FusionLayer(n_enc_1)\n",
    "        self.fuse2 = FusionLayer(n_enc_2)\n",
    "        self.fuse3 = FusionLayer(n_enc_3)\n",
    "        self.fuse4 = FusionLayer(n_z)\n",
    "\n",
    "        self.fuse5 = FusionLayer(n_dec_1)\n",
    "        self.fuse6 = FusionLayer(n_dec_2)\n",
    "        self.fuse7 = FusionLayer(n_dec_3)\n",
    "        self.fuse8 = FusionLayer(n_input)\n",
    "\n",
    "        # degree\n",
    "        self.k = k\n",
    "        self.num_bands = num_bands\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # DNN Module\n",
    "        x_bar, tra1, tra2, tra3, z, dec_1, dec_2, dec_3 = self.ael(x)\n",
    "\n",
    "        sigma = 0.2\n",
    "\n",
    "        # GCN Module\n",
    "        h = self.gnn_1(x, adj)\n",
    "        h = self.fuse1(h, tra1)\n",
    "        adj1 = dot_product(h,self.k)\n",
    "        adj1= (1 - sigma) * adj1 + sigma * adj\n",
    "        adj1_f = adj1.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A1',{'a1':adj1_f})\n",
    "        adj1 = get_Laplacian_from_weights(adj1)   #归一化\n",
    "        \n",
    "        h = self.gnn_2(h, adj1)\n",
    "        h = self.fuse2(h, tra2)\n",
    "        adj2 = dot_product(h,self.k)\n",
    "        adj2= (1 - sigma) * adj2 + sigma * adj\n",
    "        adj2_f = adj2.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A2',{'a2':adj2_f})\n",
    "        adj2 = get_Laplacian_from_weights(adj2)\n",
    "\n",
    "        \n",
    "        h = self.gnn_3(h, adj2)\n",
    "        h = self.fuse3(h, tra3)\n",
    "        adj3 = dot_product(h,self.k)\n",
    "        adj3= (1 - sigma) * adj3 + sigma * adj\n",
    "        adj3_f = adj3.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A3',{'a3':adj3_f})\n",
    "        adj3 = get_Laplacian_from_weights(adj3)\n",
    "        \n",
    "\n",
    "        h = self.gnn_4(h, adj3)\n",
    "        h = self.fuse4(h, z)\n",
    "        adj4 = dot_product(h,self.k)\n",
    "        adj4= (1 - sigma) * adj4 + sigma * adj\n",
    "        adj4_f = adj4.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A4',{'a4':adj4_f})\n",
    "        adj4 = get_Laplacian_from_weights(adj4)\n",
    "\n",
    "\n",
    "        h1 = h\n",
    "        h = self.gnn_5(h, adj4)\n",
    "        h = self.fuse5(h, dec_1)\n",
    "        adj5 = dot_product(h,self.k)\n",
    "        adj5= (1 - sigma) * adj5 + sigma * adj\n",
    "        adj5_f = adj5.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A5',{'a5':adj5_f})\n",
    "        adj5 = get_Laplacian_from_weights(adj5)\n",
    "\n",
    "        h = self.gnn_7(h, adj5)\n",
    "        h = self.fuse6(h, dec_2)\n",
    "        adj6 = dot_product(h,self.k)\n",
    "        adj6= (1 - sigma) * adj6 + sigma * adj\n",
    "        adj6_f = adj6.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A6',{'a6':adj6_f})\n",
    "        adj6 = get_Laplacian_from_weights(adj6)\n",
    "        \n",
    "\n",
    "        h = self.gnn_8(h, adj6)\n",
    "        h = self.fuse7(h, dec_3)\n",
    "        adj7 = dot_product(h,self.k)\n",
    "        adj7= (1 - sigma) * adj7 + sigma * adj\n",
    "        adj7_f = adj7.detach().cpu().numpy().astype(float)\n",
    "        sio.savemat('A7',{'a7':adj7_f})\n",
    "        adj7 = get_Laplacian_from_weights(adj7)\n",
    "\n",
    "        h = self.gnn_9(h, adj7)\n",
    "        h = self.fuse8(h, x_bar)\n",
    "        A_pred = dot_product(h,self.k)\n",
    "        A_pred = (1 - sigma) * A_pred + sigma * adj\n",
    "\n",
    "        return x_bar, z, A_pred, h, h1\n",
    "\n",
    "\n",
    "\n",
    "def get_Laplacian_from_weights(weights):\n",
    "    degree = torch.sum(weights, dim=1).pow(-0.5)\n",
    "    return (weights * degree).t()*degree\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"\n",
    "    Checks if any of the gradients are NaN, Inf\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            # Check if the gradients contain NaN or Inf\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradient for parameter {param}\")\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(f\"Inf detected in gradient for parameter {param}\")\n",
    "\n",
    "\n",
    "\n",
    "def train_dtfu(dataset, n_input, n_z, lr, name, k, epoches, pretrain_path, lamb_da):\n",
    "    num = dataset.x.shape[0]\n",
    "    model = DTFU(512, 256, 128, 128, 256, 512,\n",
    "                 n_input=n_input,\n",
    "                 n_z=n_z,\n",
    "                 pretrain_path = pretrain_path, k=k, num_bands = num).to(device)\n",
    "    print(model)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # KNN Graph\n",
    "    data = torch.Tensor(dataset.x).to(device)\n",
    "    weights, raw_weights = cal_weights_via_CAN(data.t(), k)\n",
    "    \n",
    "    weights_spectral = weights.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    spatial_matrix = spatial_similarity(data.t(),k)\n",
    "    weights_spatial = spatial_matrix.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    weights = weights * spatial_matrix\n",
    "    weights_f = weights.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    L = get_Laplacian_from_weights(weights)\n",
    "    L_f = L.detach().cpu().numpy().astype(float)\n",
    "    \n",
    "    print(\"completed!\")\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epoches):\n",
    "\n",
    "        x_bar,z, A_pred, Z, Z_ = model(data,L)\n",
    "\n",
    "        re_loss = F.mse_loss(x_bar, data)\n",
    "        re_graphloss = torch.sum(weights * torch.log(weights / (A_pred+10**-10) + 10**-10))\n",
    "        re_graphloss = re_graphloss / num\n",
    "\n",
    "        loss = lamb_da[1] * re_loss + lamb_da[0] * re_graphloss \n",
    "        losses.append(loss.item())\n",
    "        print('{} loss: {}, re_loss:{},re_graphloss:{}'.format(epoch, loss.item(),re_loss.item(),re_graphloss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        check_gradients(model)\n",
    "        optimizer.step()\n",
    "    return A_pred, Z_, Z, losses\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # name = 'DC191'\n",
    "    name = 'IP220'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Original_Img = sio.loadmat('data/DC_Sub.mat')['DC_Sub']\n",
    "    Original_Img = sio.loadmat('datasets/Indian_pines.mat')['indian_pines']\n",
    "    new_l = []\n",
    "    c = 0\n",
    "    for k in range(Original_Img.shape[2]):\n",
    "        l = []\n",
    "        for i in range(Original_Img.shape[0]):\n",
    "            for j in range(Original_Img.shape[1]):\n",
    "                l.append(Original_Img[i][j][k])\n",
    "                c = c + 1\n",
    "        new_l.append(l)\n",
    "    X = np.array(new_l)\n",
    "    \n",
    "    print('X.shape=', X.shape)\n",
    "    # X归一化\n",
    "    X_min = X.min(axis=0)  # 对每一列计算最小值\n",
    "    X_max = X.max(axis=0)  # 对每一列计算最大值\n",
    "    X = (X - X_min) / (X_max - X_min)  # 对每一列分别进行归一化\n",
    "    \n",
    "    # y = sio.loadmat('data/DC_Sub.mat')['grd']\n",
    "    y = sio.loadmat('datasets/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    y = y.reshape(X.shape[1])\n",
    "    \n",
    "    dataset = load_data(X,y)\n",
    "    pretrain_path = 'model/{}.pkl'.format(name)\n",
    "    k = 10\n",
    "    lr = 1e-4\n",
    "    n_input = X.shape[1]\n",
    "    n_z = 64\n",
    "    epoches = 100\n",
    "    \n",
    "    lamb_da = [1.0, 1000]# re_graphloss,re_loss\n",
    "    A_pred, Z_, Z, losses = train_dtfu(dataset, n_input, n_z, lr, name, k, epoches, pretrain_path, lamb_da)\n",
    "    A_pred = A_pred.detach().cpu().numpy()\n",
    "    sio.savemat('A_pred.mat',{'A': A_pred})\n",
    "    plt.plot(range(1, epoches + 1), losses, marker='o', linestyle='-', color='b', label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6636cb84-40ad-459b-86dd-4aaa6291c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 21025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:5,[15, 53, 97, 120, 205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:10,[9, 28, 52, 68, 94, 99, 122, 141, 174, 217]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:15,[3, 9, 22, 28, 39, 52, 67, 74, 94, 99, 122, 138, 143, 174, 217]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:20,[3, 6, 12, 23, 28, 34, 39, 52, 65, 74, 75, 94, 99, 101, 119, 122, 144, 167, 179, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:25,[3, 6, 12, 19, 22, 28, 32, 35, 37, 39, 52, 60, 62, 65, 73, 75, 94, 99, 101, 119, 122, 143, 167, 179, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:30,[3, 6, 12, 17, 19, 22, 28, 32, 36, 37, 39, 47, 52, 60, 61, 62, 67, 72, 75, 78, 94, 99, 101, 118, 119, 122, 144, 167, 179, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:35,[3, 5, 8, 12, 18, 25, 32, 33, 34, 35, 36, 37, 38, 40, 41, 47, 52, 60, 61, 67, 73, 74, 75, 78, 94, 99, 100, 101, 118, 119, 122, 144, 167, 179, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:40,[3, 5, 6, 8, 12, 13, 22, 26, 32, 33, 34, 35, 36, 37, 38, 40, 41, 46, 47, 52, 59, 60, 61, 62, 67, 72, 74, 75, 78, 94, 99, 100, 101, 118, 119, 122, 144, 167, 179, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:45,[3, 5, 6, 8, 12, 13, 18, 19, 22, 26, 32, 33, 34, 35, 36, 37, 38, 40, 41, 46, 47, 52, 59, 60, 61, 62, 63, 67, 73, 74, 75, 78, 90, 97, 99, 100, 101, 118, 119, 122, 144, 174, 180, 210, 218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAE\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands:50,[2, 3, 4, 5, 6, 8, 13, 14, 17, 18, 19, 22, 26, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 46, 47, 52, 57, 58, 59, 60, 62, 63, 67, 73, 74, 75, 78, 90, 97, 99, 100, 101, 118, 119, 122, 125, 144, 174, 180, 210]\n"
     ]
    }
   ],
   "source": [
    "# import scipy.io as sio\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import scipy.io as sio\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.stats import wasserstein_distance\n",
    "from joblib import Parallel, delayed  # 用于并行化\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# 计算 SSIM\n",
    "def calculate_ssim(X):\n",
    "    num_bands = X.shape[0]\n",
    "    ssim_matrix = np.zeros((num_bands, num_bands))\n",
    "\n",
    "    def compute_ssim(i, j):\n",
    "        return ssim(X[i], X[j], data_range=1)\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_ssim)(i, j) for i in range(num_bands) for j in range(i+1, num_bands))\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(num_bands):\n",
    "        for j in range(i+1, num_bands):\n",
    "            ssim_matrix[i, j] = results[idx]\n",
    "            ssim_matrix[j, i] = results[idx]  # 对称填充下三角矩阵\n",
    "            idx += 1\n",
    "            \n",
    "    return ssim_matrix\n",
    "\n",
    "# 计算 IcSDD\n",
    "def calculate_icSDD(X, grp, ssim_matrix):\n",
    "    num_bands = X.shape[0]\n",
    "    icSDDs = np.zeros(num_bands)\n",
    "\n",
    "    for i in range(num_bands):\n",
    "        same_cluster = grp == grp[i]  # 同簇波段\n",
    "        different_cluster = grp != grp[i]  # 不同簇波段\n",
    "\n",
    "        positive_similarities = ssim_matrix[i, same_cluster]\n",
    "        negative_similarities = ssim_matrix[i, different_cluster]\n",
    "\n",
    "        # 计算 IcSDD\n",
    "        icSDD = wasserstein_distance(positive_similarities, negative_similarities)\n",
    "        icSDDs[i] = icSDD\n",
    "\n",
    "    return icSDDs\n",
    "\n",
    "# 计算特征矩阵的熵\n",
    "def Entrop(X):\n",
    "    G = 256\n",
    "    L, N = X.shape\n",
    "    rak_val = np.zeros(L)\n",
    "    minX = np.min(X)\n",
    "    maxX = np.max(X)\n",
    "    edge = np.linspace(minX, maxX, G)\n",
    "    for i in range(L):\n",
    "        histX, _ = np.histogram(X[i, :], bins=edge, density=True)\n",
    "        rak_val[i] = -np.sum(histX * np.log2(histX + np.finfo(float).eps))\n",
    "    return rak_val\n",
    "\n",
    "# 计算标准拉普拉斯矩阵\n",
    "def compute_laplacian(W):\n",
    "    D = np.diag(W.sum(axis=1))  # 度矩阵，D[i, i] 是第 i 个节点的度数\n",
    "    L = D - W  # 标准拉普拉斯矩阵\n",
    "    return L\n",
    "\n",
    "# 特征值分解并选择 k 个最小特征值对应的特征向量\n",
    "def compute_eigenvectors(L, k):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    return eigenvectors[:, :k]\n",
    "\n",
    "# 使用 K-means 聚类\n",
    "def perform_kmeans(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "    return kmeans.fit_predict(X)\n",
    "\n",
    "# 主函数\n",
    "def spectral_clustering(W, n_clusters):\n",
    "    L = compute_laplacian(W)\n",
    "    k = n_clusters  # 聚类数就是选取的特征向量个数\n",
    "    eigenvectors = compute_eigenvectors(L, k)\n",
    "    labels = perform_kmeans(eigenvectors, n_clusters)\n",
    "    return labels\n",
    "\n",
    "# 数据预处理\n",
    "# Original_Img = sio.loadmat('data/DC_Sub.mat')['DC_Sub']\n",
    "Original_Img = sio.loadmat('datasets/Indian_pines.mat')['indian_pines']\n",
    "\n",
    "new_l = []\n",
    "c = 0\n",
    "for k in range(Original_Img.shape[2]):\n",
    "    l = []\n",
    "    for i in range(Original_Img.shape[0]):\n",
    "        for j in range(Original_Img.shape[1]):\n",
    "            l.append(Original_Img[i][j][k])\n",
    "            c = c + 1\n",
    "    new_l.append(l)\n",
    "\n",
    "X = np.array(new_l)\n",
    "print(X.shape)\n",
    "\n",
    "# 对每一列进行归一化\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "W = sio.loadmat('A_pred')['A']\n",
    "W = (W + W.T) / 2\n",
    "\n",
    "\n",
    "with open(\"selected_bands.txt\", \"w\") as file:\n",
    "    for num_bands in [5,10,15,20,25,30,35,40,45,50]:\n",
    "    \n",
    "        grp = spectral_clustering(W, num_bands)\n",
    "        \n",
    "        # 计算 SSIM\n",
    "        ssim_matrix = calculate_ssim(X)\n",
    "        \n",
    "        # 计算 IcSDD 和熵\n",
    "        icSDDs = calculate_icSDD(X, grp, ssim_matrix)\n",
    "        entropies = Entrop(X)\n",
    "        \n",
    "        # 归一化 IcSDD 和熵\n",
    "        icSDDs = (icSDDs - icSDDs.min()) / (icSDDs.max() - icSDDs.min())\n",
    "        entropies = (entropies - entropies.min()) / (entropies.max() - entropies.min())\n",
    "        \n",
    "        # 合并值\n",
    "        values = icSDDs + entropies\n",
    "        \n",
    "        # 获取每个类别的最大值及其索引\n",
    "        max_values = {}\n",
    "        for i, label in enumerate(grp):\n",
    "            if label not in max_values or values[i] > max_values[label][0]:\n",
    "                max_values[label] = (values[i], i)\n",
    "        \n",
    "        # 存储每个类别最大值样本的索引\n",
    "        selected_bands = [index for _, index in max_values.values()]\n",
    "        selected_bands.sort()\n",
    "        selected_bands = [x+1 for x in selected_bands]\n",
    "    \n",
    "        print(f\"num_bands:{num_bands},{selected_bands}\")\n",
    "        file.write(f\"num_bands:{num_bands}, {selected_bands}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cd878-2bf3-4c2a-bdda-b6d64a443210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python GAE",
   "language": "python",
   "name": "gae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
